from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def stream_data_function():
    import json
    import requests
    from kafka import KafkaProducer
    import time
    import logging

    # Kh·ªüi t·∫°o Kafka Producer (ch·ªâ 1 l·∫ßn, b√™n ngo√†i v√≤ng l·∫∑p)
    try:
        producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],  # broker c·ªßa Kafka
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        logging.info("Kafka Producer initialized successfully.")
    except Exception as e:
        logging.error(f"Failed to initialize Kafka Producer: {e}")
        return  # Tho√°t n·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Kafka

    print("üîπ Starting data streaming for 60 seconds...")
    
    # L·∫•y th·ªùi gian b·∫Øt ƒë·∫ßu
    curr_time = time.time()

    while True:
        # ƒêi·ªÅu ki·ªán d·ª´ng: Ch·∫°y trong 60 gi√¢y
        if time.time() > curr_time + 60:
            print("üõë 60 seconds elapsed. Stopping stream.")
            break

        try:
            # G·ªçi API
            response = requests.get('https://randomuser.me/api/')
            
            if response.status_code == 200:
                data = response.json()
                
                producer.send('user_data', value=data)
                print("‚úÖ Sent data to Kafka topic: user_data")
            
            else:
                logging.warning(f"Failed to fetch data, status_code={response.status_code}")

        except Exception as e:
            logging.error(f"An error occurred: {e}")
            
        time.sleep(1) 

    producer.flush()
    producer.close()
    print("üîπ Data streaming finished.")


with DAG('user_automation',
         default_args=default_args,
         description='A simple user automation DAG',
         schedule='@daily',
         catchup=False) as dag:

    stream_data = PythonOperator(
        task_id='stream_data',
        python_callable=stream_data_function,
    )